#!/usr/bin/env python3
"""
auto_maker_scraper.py â€“ 2025-09-09 r2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Carwow ã‹ã‚‰ â€œç¾å­˜ã™ã‚‹å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼â€ ã‚’æŠ½å‡º â†’ ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¿”ã™ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
  * /brands, ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸, robots.txt / sitemap ã®ï¼“æ®µæ§‹ãˆ
  * ç”Ÿæˆçµæœã‚’ GitHub Actions ã§ãã®ã¾ã¾ç’°å¢ƒå¤‰æ•°ã¸æµç”¨ã—ã‚„ã™ã„å½¢å¼ã§å‡ºåŠ›
ä¾å­˜: requests, beautifulsoup4 (ï¼‹lxml)
"""

from __future__ import annotations
import re, sys, time
from typing import List, Set
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

BASE = "https://www.carwow.co.uk"
HEADERS = {"User-Agent": "Mozilla/5.0 (carwow-auto-maker-scraper/2025)"}
TIMEOUT = 20

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper
def _extract_maker(url: str) -> str | None:
    """URL æ–‡å­—åˆ—ã‹ã‚‰ maker åã‚’æ¨å®šã—ã¦è¿”ã™ï¼ˆä¸é©åˆ‡ãªã‚‰ Noneï¼‰"""
    if not url:
        return None
    if url.startswith("/"):
        url = urljoin(BASE, url)
    try:
        path = urlparse(url).path.strip("/").lower()
    except Exception:
        return None
    part = path.split("/", 1)[0]
    if len(part) < 2 or len(part) > 20:
        return None
    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰
    if part in {
        "news", "blog", "help", "sell", "compare", "tools", "brands",
        "finance", "insurance", "lease", "deals", "search", "about",
        "reviews", "electric", "hybrid", "suv", "hatchback", "saloon",
        "estate", "coupe", "convertible", "used", "new",
    }:
        return None
    if not re.fullmatch(r"[a-z\-]+", part):
        return None
    return part

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main scraper
class MakerScraper:
    def __init__(self):
        self.s = requests.Session()
        self.s.headers.update(HEADERS)

    # 1) /brands ãƒšãƒ¼ã‚¸
    def from_brands(self) -> Set[str]:
        makers: Set[str] = set()
        try:
            r = self.s.get(f"{BASE}/brands", timeout=TIMEOUT)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "lxml")
            for a in soup.select('a[href*="/brands/"]'):
                m = _extract_maker(a.get("href", ""))
                if m:
                    makers.add(m)
        except Exception as e:
            print(f"[warn] /brands fetch failed: {e}", file=sys.stderr)
        return makers

    # 2) ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
    def from_home(self) -> Set[str]:
        makers: Set[str] = set()
        try:
            r = self.s.get(BASE, timeout=TIMEOUT)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "lxml")
            for a in soup.find_all("a", href=True):
                m = _extract_maker(a["href"])
                if m:
                    makers.add(m)
        except Exception as e:
            print(f"[warn] / homepage fetch failed: {e}", file=sys.stderr)
        return makers

    # 3) robots.txt / sitemap
    def from_sitemap(self) -> Set[str]:
        makers: Set[str] = set()
        for url in (f"{BASE}/robots.txt",
                    f"{BASE}/sitemap.xml",
                    f"{BASE}/sitemap_index.xml"):
            try:
                r = self.s.get(url, timeout=TIMEOUT)
                if not r.ok:
                    continue
                urls = re.findall(r"https?://[^\s<>\"']+", r.text)
                for u in urls:
                    m = _extract_maker(u)
                    if m:
                        makers.add(m)
                if makers:
                    break
            except Exception:
                continue
        return makers

    def all_makers(self) -> List[str]:
        makers: Set[str] = set()
        makers.update(self.from_brands())
        makers.update(self.from_home())
        makers.update(self.from_sitemap())
        # æ­£è¦åŒ– & ã‚½ãƒ¼ãƒˆ
        return sorted(makers)


def main() -> None:
    print("ğŸš—  Discovering all makers from carwowâ€¦", file=sys.stderr)
    start = time.time()
    scraper = MakerScraper()
    makers = scraper.all_makers()
    elapsed = time.time() - start
    # â”€â”€ human readable
    print(f"\nâœ…  Found {len(makers)} makers in {elapsed:.1f}s")
    for i, m in enumerate(makers, 1):
        print(f"{i:>2}  {m}")
    # â”€â”€ GitHub Actions ç”¨
    joined = " ".join(makers)
    print(f'\nMAKES_FOR_BODYMAP: "{joined}"')
    # stdout ã¸ makers ã ã‘åãã€å‘¼ã³å‡ºã—å´ãŒæ‹¾ã†
    print(joined)


if __name__ == "__main__":
    main()
