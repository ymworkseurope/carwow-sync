#!/usr/bin/env python3
"""
auto_maker_scraper.py ‚Äì 2025‚Äë09‚Äë08  ‚ú®FULL VERSION‚ú®
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Carwow „Åã„Çâ *Âà©Áî®ÂèØËÉΩ„Å™ÂÖ®„É°„Éº„Ç´„Éº* „ÇíÊäΩÂá∫„Åó„Å¶„ÄÅ
GitHub Actions „ÅÆÁí∞Â¢ÉÂ§âÊï∞ `MAKES_FOR_BODYMAP` Âêë„Åë„Å´
„Çπ„Éö„Éº„ÇπÂå∫Âàá„ÇäÊñáÂ≠óÂàó„ÇíÂá∫Âäõ„Åó„Åæ„Åô„ÄÇ

‰∏ª„Å™ÁâπÈï∑
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. **brands „Éö„Éº„Ç∏„Éª„Éà„ÉÉ„Éó„Éö„Éº„Ç∏„Éª„Çµ„Ç§„Éà„Éû„ÉÉ„Éó** „ÅÆ 3 ÊÆµÊäΩÂá∫„ÅßÂèñ„ÇäÊºè„Çâ„Åó„ÇíÊúÄÂ∞èÂåñ„ÄÇ
2. Èô§Â§ñ„Ç≠„Éº„ÉØ„Éº„ÉâÔºèÊ≠£Ë¶èË°®Áèæ„Åß‚Äú„É¢„Éá„É´Âêç‚Äù„ÇÑÊ±éÁî®„Éö„Éº„Ç∏„ÇíËá™Âãï„Éï„Ç£„É´„Çø„ÄÇ
3. `--short` „Ç™„Éó„Ç∑„Éß„É≥„Åß *Áí∞Â¢ÉÂ§âÊï∞Âá∫Âäõ„ÅÆ„Åø*„ÄÅ‰Ωï„ÇÇ‰ªò„Åë„Å™„Åë„Çå„Å∞„ÉÜ„Éº„Éñ„É´Ë°®Á§∫‰ªò„Åç„ÄÇ
4. UA ÂÅΩË£ÖÔºÜ„É™„Éà„É©„Ç§‰ªò„Åç `requests.Session`„ÄÇ

ÂøÖË¶Å„É©„Ç§„Éñ„É©„É™: `requests`, `beautifulsoup4`, `lxml`Ôºà„Ç§„É≥„Çπ„Éà„Éº„É´Ê∏à„Åø„Å™„Çâ OKÔºâ
"""
from __future__ import annotations

import argparse
import re
import sys
import textwrap
from html import unescape
from typing import List, Set
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

BASE_URL = "https://www.carwow.co.uk"
HEADERS = {"User-Agent": "Mozilla/5.0 (carwow-auto-maker/1.0)"}
TIMEOUT = 30
EXCLUDE = {
    "news",
    "blog",
    "lease",
    "deals",
    "finance",
    "insurance",
    "sell",
    "electric",
    "hybrid",
    "suv",
    "hatchback",
    "saloon",
    "used",
    "new",
    "reviews",
    "review",
    "search",
    "tools",
    "help",
    "compare",
    "brands",
    "cars",
    "advice",
    "terms",
    "privacy",
    "about",
    "contact",
    "cookie",
}


class MakerScraper:
    def __init__(self) -> None:
        s = requests.Session()
        s.headers.update(HEADERS)
        self.sess = s

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ internal helpers

    def _clean(self, name: str) -> str:
        return unescape(name.strip().lower())

    def _valid(self, name: str) -> bool:
        return (
            name
            and name not in EXCLUDE
            and 1 < len(name) < 20
            and re.fullmatch(r"[a-z\-]+", name) is not None
        )

    def _extract_from_href(self, href: str) -> str | None:
        if not href:
            return None
        # Áõ∏ÂØæ‚ÜíÁµ∂ÂØæÂåñ
        if href.startswith("/"):
            href = urljoin(BASE_URL, href)
        parsed = urlparse(href)
        parts = [p for p in parsed.path.split("/") if p]
        if parts:
            cand = self._clean(parts[0])
            return cand if self._valid(cand) else None
        return None

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3 sources

    def from_brands(self) -> Set[str]:
        url = f"{BASE_URL}/brands"
        try:
            r = self.sess.get(url, timeout=TIMEOUT)
            r.raise_for_status()
        except Exception as e:
            print(f" ‚úó /brands error: {e}", file=sys.stderr)
            return set()
        soup = BeautifulSoup(r.text, "lxml")
        makers = {
            self._extract_from_href(a.get("href", ""))
            for a in soup.select('a[href^="/brands/"]')
        }
        makers.discard(None)
        return set(makers)  # type: ignore

    def from_home(self) -> Set[str]:
        try:
            r = self.sess.get(BASE_URL, timeout=TIMEOUT)
            r.raise_for_status()
        except Exception as e:
            print(f" ‚úó / home error: {e}", file=sys.stderr)
            return set()
        soup = BeautifulSoup(r.text, "lxml")
        makers = {self._extract_from_href(a.get("href", "")) for a in soup.find_all("a")}
        makers.discard(None)
        return set(makers)  # type: ignore

    def from_sitemap(self) -> Set[str]:
        sitemap_urls = [
            f"{BASE_URL}/sitemap.xml",
            f"{BASE_URL}/sitemap_index.xml",
            f"{BASE_URL}/robots.txt",
        ]
        makers: Set[str] = set()
        for sm in sitemap_urls:
            try:
                r = self.sess.get(sm, timeout=TIMEOUT)
                if not r.ok:
                    continue
                urls = re.findall(r"https?://[^\s<>\"]+", r.text)
                for u in urls:
                    cand = self._extract_from_href(u)
                    if cand:
                        makers.add(cand)
            except Exception:
                continue
        return makers

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ public facade

    def all_makers(self) -> List[str]:
        makers: Set[str] = set()
        makers |= self.from_brands()
        makers |= self.from_home()
        makers |= self.from_sitemap()
        return sorted(makers)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI

def main() -> None:
    ap = argparse.ArgumentParser(description="Fetch all car makes from carwow.co.uk")
    ap.add_argument(
        "--short",
        action="store_true",
        help="print space‚Äëseparated list for GitHub env (no extra output)",
    )
    args = ap.parse_args()

    scraper = MakerScraper()
    makers = scraper.all_makers()

    if args.short:
        print(" ".join(makers))
        return

    print("\nüöó  Carwow ‚Äî ALL MAKES ({} total)".format(len(makers)))
    print("=" * 50)
    for i, mk in enumerate(makers, 1):
        print(f"{i:2d}. {mk}")
    print("\nüìù GitHub Actions export: \nMAKES_FOR_BODYMAP=\"{}\"".format(" ".join(makers)))


if __name__ == "__main__":
    main()
