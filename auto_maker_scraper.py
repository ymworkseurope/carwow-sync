#!/usr/bin/env python3
"""
auto_maker_scraper.py â€” 2025â€‘09â€‘08
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Carwow ã‹ã‚‰ã€ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ã€ã‚’æŠ½å‡ºã—ã¦ **MAKES_FOR_BODYMAP**
ç’°å¢ƒå¤‰æ•°ã®å€¤ã«ä½¿ãˆã‚‹å½¢ã§å‡ºåŠ›ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚

â€¢ ä¾å­˜: requests, beautifulsoup4, lxml (åŒã˜ venv ã§å‹•ä½œ)
â€¢ ä½¿ã„æ–¹ (ãƒ­ãƒ¼ã‚«ãƒ«):  python auto_maker_scraper.py > makers.txt
â€¢ ä½¿ã„æ–¹ (GitHub Actions å†…):
    - run: |
        MAKES=$(python scripts/auto_maker_scraper.py --short)
        echo "MAKES_FOR_BODYMAP=$MAKES" >> "$GITHUB_ENV"

ç”Ÿæˆã•ã‚Œã‚‹ãƒ¡ãƒ¼ã‚«ãƒ¼åã¯ carwow ã® URL ã§ä½¿ã‚ã‚Œã‚‹ slug ãã®ã¾ã¾
(ä¾‹: "alfa-romeo", "mercedes", "rolls-royce")ã€‚

ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã¯ä»¥ä¸‹ã® 3 ã¤ã®ã‚½ãƒ¼ã‚¹ã‚’é †ç•ªã«è©¦ã—ã¦çµ±åˆã—ã¾ã™ã€‚
 1. https://www.carwow.co.uk/brands
 2. https://www.carwow.co.uk (ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å†…ãƒªãƒ³ã‚¯)
 3. robots.txt / sitemap.xml ç¾¤ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
"""

from __future__ import annotations

import re
import sys
import textwrap
from typing import List, Set
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://www.carwow.co.uk"
UA = "Mozilla/5.0 (carwow-auto-maker-scraper/2025)"
TIMEOUT = 30

# carwow ä»¥å¤–ã® URL ãŒæ··å…¥ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’é˜²ã â€” host åãŒ BASE_URL ã¨åŒã˜ã‹åˆ¤å®š
def _same_host(url: str) -> bool:
    return urlparse(url).netloc.replace("www.", "") == urlparse(BASE_URL).netloc.replace("www.", "")

class CarwowMakerScraper:
    """Carwow ã‹ã‚‰å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ slug ã‚’æŠ½å‡ºã™ã‚‹"""

    #: ãƒ¡ãƒ¼ã‚«ãƒ¼ã§ã¯ã‚ã‚Šå¾—ãªã„èªå¥ (é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
    _EXCLUDED = {
        "news",
        "blog",
        "deals",
        "finance",
        "insurance",
        "sell",
        "electric",
        "hybrid",
        "suv",
        "hatchback",
        "saloon",
        "used",
        "new",
        "lease",
        "pcp",
        "reviews",
        "advice",
        "about",
        "contact",
        "help",
        "terms",
        "privacy",
        "brands",
        "cars",
        "search",
        "compare",
        "tools",
    }

    def __init__(self) -> None:
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": UA})

    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_all_makers(self) -> List[str]:
        makers: Set[str] = set()

        makers.update(self._from_brands_page())
        makers.update(self._from_homepage())
        makers.update(self._from_sitemaps())

        return sorted(self._filter_normalize(makers))

    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ private helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _from_brands_page(self) -> Set[str]:
        makers: Set[str] = set()
        try:
            print("ğŸ“‹ /brands ãƒšãƒ¼ã‚¸ã‚’è§£æä¸­â€¦", file=sys.stderr)
            soup = self._get_soup(f"{BASE_URL}/brands")
            links = soup.select('a[href^="/brands/"]')
            for a in links:
                href = a.get("href", "")
                maker = self._extract_maker(href)
                if maker:
                    makers.add(maker)
            print(f"  â†³ {len(makers)}Â found on /brands", file=sys.stderr)
        except Exception as e:
            print(f"  âœ— brands page: {e}", file=sys.stderr)
        return makers

    def _from_homepage(self) -> Set[str]:
        makers: Set[str] = set()
        try:
            print("ğŸ  ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚’è§£æä¸­â€¦", file=sys.stderr)
            soup = self._get_soup(BASE_URL)
            for a in soup.find_all("a", href=True):
                href = a["href"]
                maker = self._extract_maker(href)
                if maker:
                    makers.add(maker)
            print(f"  â†³ {len(makers)}Â found on homepage", file=sys.stderr)
        except Exception as e:
            print(f"  âœ— homepage: {e}", file=sys.stderr)
        return makers

    def _from_sitemaps(self) -> Set[str]:
        makers: Set[str] = set()
        print("ğŸ—ºï¸  ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è§£æä¸­â€¦", file=sys.stderr)
        cand = [
            f"{BASE_URL}/sitemap.xml",
            f"{BASE_URL}/sitemap_index.xml",
            f"{BASE_URL}/robots.txt",
        ]
        for url in cand:
            try:
                r = self.session.get(url, timeout=TIMEOUT)
                if not r.ok:
                    continue
                for loc in re.findall(r"https?://[^\s<>\"]+", r.text):
                    maker = self._extract_maker(loc)
                    if maker:
                        makers.add(maker)
            except Exception:
                continue
        print(f"  â†³ {len(makers)}Â found via sitemaps", file=sys.stderr)
        return makers

    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTML fetch & parse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _get_soup(self, url: str) -> BeautifulSoup:
        r = self.session.get(url, timeout=TIMEOUT)
        r.raise_for_status()
        return BeautifulSoup(r.text, "lxml")

    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ extract & validate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _extract_maker(self, href: str) -> str | None:
        if not href:
            return None
        # çµ¶å¯¾ URL ã«å¤‰æ›
        if href.startswith("/"):
            href = urljoin(BASE_URL, href)
        if not _same_host(href):
            return None
        path = urlparse(href).path.strip("/")
        if not path:
            return None
        maker = path.split("/")[0].lower()
        return maker if self._is_valid_maker(maker) else None

    def _is_valid_maker(self, name: str) -> bool:
        if name in self._EXCLUDED:
            return False
        if len(name) < 2 or len(name) > 20:
            return False
        # è‹±å°æ–‡å­—ã¨ãƒã‚¤ãƒ•ãƒ³ã®ã¿è¨±å¯
        return bool(re.match(r"^[a-z-]+$", name))

    def _filter_normalize(self, makers: Set[str]) -> List[str]:
        # è¿½åŠ ã®å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ã“ã“ã§å®Ÿæ–½å¯
        return [m.lower().strip() for m in makers if self._is_valid_maker(m)]

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI å…¥å£
#   â€¢ --short   : ç©ºç™½åŒºåˆ‡ã‚Šã®ã¿ (GitHubÂ Actions ç”¨)
#   â€¢ ãã‚Œä»¥å¤– : äººé–“å‘ã‘ãƒªã‚¹ãƒˆ & Actions ç”¨ã® export è¡Œ ã‚’å‡ºåŠ›
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    short = "--short" in sys.argv
    scraper = CarwowMakerScraper()
    makers = scraper.get_all_makers()

    if short:
        print(" ".join(makers))
        sys.exit(0)

    print("\nğŸ‰ å–å¾—å®Œäº† â€” å…¨ãƒ¡ãƒ¼ã‚«ãƒ¼ä¸€è¦§ ({} ä»¶)".format(len(makers)))
    for i, mk in enumerate(makers, 1):
        print(f"{i:>2}. {mk}")

    print("\nğŸ“ GitHub Actions ç”¨ (ç’°å¢ƒå¤‰æ•°ã«ã‚³ãƒ”ãƒš) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f'MAKES_FOR_BODYMAP: "{" ".join(makers)}"')
