name: Daily Carwow Sync

on:
  # 日本時間 AM2:00 （= UTC 17:00）に毎日実行
  schedule:
    - cron: '0 17 * * *'
  workflow_dispatch:          # 手動トリガーも可

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180      # 実行上限を少し延長

    env:
      # ─── Secrets ───────────────────────────────────────────────
      SUPABASE_URL:  ${{ secrets.SUPABASE_URL }}
      SUPABASE_KEY:  ${{ secrets.SUPABASE_KEY }}
      DEEPL_KEY:     ${{ secrets.DEEPL_KEY }}
      GS_CREDS_JSON: ${{ secrets.GS_CREDS_JSON }}
      GS_SHEET_ID:   ${{ secrets.GS_SHEET_ID }}
      # ─── その他パラメータ ─────────────────────────────────────
      GBP_TO_JPY:    "195"        # 総当り換算用
      DEBUG_MODE:    "false"      # true にするとスクレイパが 3 車種だけ回す
      MAKES_FOR_BODYMAP: ""       # 空なら「サイトマップ全メーカー」を自動検出

    steps:
    # ① ソース取得
    - name: Checkout code
      uses: actions/checkout@v4

    # ② Python 3.11
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    # ③ 依存パッケージ
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # ④ Chrome / ChromeDriver (Selenium 用)
    - name: Setup Chrome
      uses: browser-actions/setup-chrome@latest
    - name: Setup ChromeDriver
      uses: nanasess/setup-chromedriver@v2

    # ⑤ Google サービスアカウント
    - name: Setup Google credentials
      if: env.GS_CREDS_JSON != ''
      run: |
        mkdir -p secrets
        echo "$GS_CREDS_JSON" > secrets/gs_creds.json

    # ⑥   ────────────────────────────────────────────────────────
    #      “メーカー一覧” を決定
    #      ────────────────────────────────────────────────────────
    - name: Detect makes (from sitemap when env empty)
      id: makelist
      run: |
        if [[ -n "$MAKES_FOR_BODYMAP" ]]; then
          # 1) 環境変数に手動指定があればそれを使う
          MAKES="$MAKES_FOR_BODYMAP"
        else
          # 2) 空なら …/makes-sitemap.xml から自動抽出
          python - <<'PY'
          import os, re, requests, textwrap
          SM_URL = "https://www.carwow.co.uk/sitemap/makes-sitemap.xml"
          xml = requests.get(SM_URL, timeout=20).text
          makes = sorted(set(re.findall(r"https://www\.carwow\.co\.uk/([^/]+)/", xml)))
          # colour など明らかにモデルではない slug は除外
          skip = {
              "colour", "color", "lease", "news", "review",
              "used", "deals", "commercial", "van"
          }
          makes = [m for m in makes if m not in skip]
          makes_str = " ".join(makes)
          print(f"▸ Found {len(makes)} makes")
          # GitHub Actions へ出力
          with open(os.environ["GITHUB_OUTPUT"], "a") as fh:
              fh.write(f"makes={makes_str}\n")
          PY
          MAKES="${{ steps.makelist.outputs.makes }}"
        fi
        echo "MAKES=${MAKES}"
        echo "makes=${MAKES}" >> "$GITHUB_OUTPUT"

    # ⑦ body_type_mapper.py を全メーカー分まわす
    - name: Build body_type maps
      run: |
        IFS=' ' read -ra MAKES <<< "${{ steps.makelist.outputs.makes }}"
        for make in "${MAKES[@]}"; do
          echo "▶ Building body map for ${make}…"
          python body_type_mapper.py "${make}" \
            || echo "⚠️  ${make}: failed (skip)"
        done

    # ⑧ メインクローラー
    - name: Run main crawler
      run: |
        echo "Starting crawler at $(date)"
        python scrape.py             # slug 指定無し → body_map_* を全読込
        echo "Crawler finished at $(date)"

    # ⑨ 生成物を Artifacts へ
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawler-logs-${{ github.run_number }}
        path: |
          *.log
          *.jsonl
          body_map_*.json
        retention-days: 7

    # ⑩ 失敗時通知（任意）
    - name: Notify on failure
      if: failure()
      run: |
        echo "Crawler failed - check logs."
        # Slack / Discord などの通知が必要ならここに書く
